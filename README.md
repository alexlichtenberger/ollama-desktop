# Ollama Desktop/Studio
A quickly thrown together front end for managing and interacting with LLMs locally using Ollama.

Not all the buttons work, just a heads up.


### usage
Select a model under Settings>ollama>model, then chat in the window to the right.
Currently you will need to pull models using the ollama CLI before you can use them here.

### installation
1. Clone Repositiory
2. `npm i`

### to run in dev mode
`npm run tauri dev`

### to build an installer
`npm run tauri build`